{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aec82e69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "if '__file__' in globals():\n",
    "    # Navigate up 4 levels from file location to repo root\n",
    "    repo_root = Path(__file__).resolve().parent.parent.parent.parent\n",
    "else:\n",
    "    # Test two execution scenarios\n",
    "    cwd = Path.cwd()\n",
    "    if (cwd / 'src').exists():\n",
    "        # Executing from repo root (right above src)\n",
    "        repo_root = cwd\n",
    "    else:\n",
    "        # Executing from notebooks directory, go up 3 levels\n",
    "        repo_root = cwd.parent.parent.parent\n",
    "\n",
    "repo_root_path = str(repo_root.resolve())\n",
    "if repo_root_path not in sys.path:\n",
    "    sys.path.insert(0, repo_root_path)\n",
    "    print(f'{repo_root_path} added to sys path')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae75f8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from src.etl.common.config import *\n",
    "    from src.etl.common.table_updater import *\n",
    "except:\n",
    "    from config import *\n",
    "    from table_updater import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c2eed8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ETL DAG Orchestrator\n",
    "\n",
    "Creates a DAG that orchestrates ETL loads in dependency order:\n",
    "1. dim_employee (runs first)\n",
    "2. fact_employee_pay (runs after dim_employee succeeds)\n",
    "\n",
    "Usage:\n",
    "    Copy this code into a Snowflake Python worksheet or notebook cell,\n",
    "    adjust the configuration variables, and run.\n",
    "\"\"\"\n",
    "\n",
    "from snowflake.snowpark import Session\n",
    "from snowflake.core.task.dagv1 import DAG, DAGTask, DAGOperation\n",
    "from snowflake.core import CreateMode, Root\n",
    "\n",
    "# =============================================================================\n",
    "# Configuration - adjust these values for your environment\n",
    "# =============================================================================\n",
    "TARGET_DATABASE = 'LEARNING_DB'\n",
    "TARGET_SCHEMA = 'ETL'\n",
    "WAREHOUSE_NAME = 'COMPUTE_WH'\n",
    "\n",
    "# Set to number of minutes for automatic scheduling, or None for manual execution only\n",
    "SCHEDULE_MINUTES = None  # e.g., 60 for hourly runs\n",
    "\n",
    "\n",
    "def create_dag_orchestrator(session: Session) -> str:\n",
    "    \"\"\"\n",
    "    Creates and deploys the ETL DAG orchestrator.\n",
    "    \n",
    "    Args:\n",
    "        session: Snowflake session object (use get_active_session() in notebooks)\n",
    "    \n",
    "    Returns:\n",
    "        Success message with DAG details\n",
    "    \"\"\"\n",
    "    root = Root(session)\n",
    "    \n",
    "    # Build DAG configuration\n",
    "    dag_config = {\n",
    "        'name': 'etl_dag_orchestrator_notebook_test',\n",
    "        'warehouse': WAREHOUSE_NAME\n",
    "    }\n",
    "    \n",
    "    # Add schedule if configured\n",
    "    if SCHEDULE_MINUTES:\n",
    "        dag_config['schedule'] = f'{SCHEDULE_MINUTES} MINUTE'\n",
    "    \n",
    "    # Define the DAG with tasks and dependencies\n",
    "    with DAG(**dag_config) as dag:\n",
    "        # Task 1: Load dim_employee (runs first)\n",
    "        task_dim_employee = DAGTask(\n",
    "            name=\"load_dim_employee\",\n",
    "            definition=f\"EXECUTE NOTEBOOK learning_db.etl.MAIN_SRC_ETL_NOTEBOOKS_TEST_NOTEBOOK_WITH_COMMON;\",\n",
    "            comment=\"Load dimension table: dim_employee\"\n",
    "        )\n",
    "    \n",
    "    # Deploy the DAG\n",
    "    dag_op = DAGOperation(root.databases[TARGET_DATABASE].schemas[TARGET_SCHEMA])\n",
    "    dag_op.deploy(dag, mode=CreateMode.or_replace)\n",
    "    \n",
    "    schedule_msg = f\" (scheduled every {SCHEDULE_MINUTES} minutes)\" if SCHEDULE_MINUTES else \" (manual execution only)\"\n",
    "    return f\"DAG 'etl_dag_orchestrator' deployed to {TARGET_DATABASE}.{TARGET_SCHEMA}{schedule_msg}\"\n",
    "\n",
    "\n",
    "def execute_dag(session: Session) -> str:\n",
    "    \"\"\"Manually trigger the DAG execution.\"\"\"\n",
    "    root = Root(session)\n",
    "    tasks = root.databases[TARGET_DATABASE].schemas[TARGET_SCHEMA].tasks\n",
    "    dag_task = tasks['etl_dag_orchestrator']\n",
    "    dag_task.execute()\n",
    "    return \"DAG execution triggered!\"\n",
    "\n",
    "\n",
    "session = get_session()\n",
    "    \n",
    "# Deploy the DAG\n",
    "result = create_dag_orchestrator(session)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f3db5a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "root = Root(session)\n",
    "tasks = root.databases[\"LEARNING_DB\"].schemas[\"ETL\"].tasks\n",
    "dag_res = tasks['etl_dag_orchestrator_notebook_test']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cda9dc4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dag_res.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "snowflake_demo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
